{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cloud_shell.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPrVPrDyvaSnz4gyl3vRmwu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/catherinedoudou/BeautifulSoup-Waterstones/blob/main/cloud_shell.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()    ##  get authentiaction from colab"
      ],
      "metadata": {
        "id": "6ZezMPoukPAJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests_html > /dev/null"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YonCEFCkltv",
        "outputId": "d1f74784-4f6d-4428-e8ff-65e825e0a0ca"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "DMYdZClNLAZ1"
      },
      "outputs": [],
      "source": [
        "# from to_cloud import upload_to_bigquery, upload_to_gcs\n",
        "import os\n",
        "from requests_html import HTMLSession\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "from datetime import datetime\n",
        "from google.cloud import bigquery\n",
        "from google.cloud import storage\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from io import BytesIO\n",
        "import io\n",
        "from PIL import Image\n",
        "import time\n",
        "\n",
        "\n",
        "def upload_to_bigquery(row_to_insert, table_id):\n",
        "    # Instantiates a client\n",
        "    bigquery_client = bigquery.Client(project='catherine-test-347809')\n",
        "\n",
        "    # Prepares a reference to the dataset\n",
        "    dataset_ref = bigquery_client.dataset('company')\n",
        "\n",
        "    # Gets the table \n",
        "    table_ref = dataset_ref.table(table_id)\n",
        "    table = bigquery_client.get_table(table_ref)  # API call\n",
        "\n",
        "    errors = bigquery_client.insert_rows(table, row_to_insert)  # API request\n",
        "    assert errors == []\n",
        "\n",
        "\n",
        "def upload_to_gcs(bucket_name, folder_name, file_name): # Retrieve an existing bucket\n",
        "\n",
        "  storage_client = storage.Client()\n",
        "  bucket = storage_client.get_bucket(bucket_name)\n",
        "\n",
        "  new_blob = bucket.blob(f'{folder_name}/{file_name}')  # folder_name/logo_name\n",
        "  new_blob.upload_from_filename(f'/content/{file_name}')     # file_name looks like 'logo.png'\n",
        "  \n",
        "  if os.path.exists(file_name):     # delete the local file immediately after upload images into gcs succeed\n",
        "    os.remove(file_name)\n",
        "  else:\n",
        "    print(\"The file does not exist\")\n",
        "\n",
        "\n",
        "def get_soup(url):\n",
        "    cert = \"zyte-smartproxy-ca.crt\"\n",
        "\n",
        "    ### Proxy settings\n",
        "    proxy_host = \"proxy.crawlera.com\"\n",
        "    proxy_port = \"8011\"\n",
        "    proxy_auth = \"f6b775febdec4c9b951e9097883fb411:\"\n",
        "    proxies = {\n",
        "        \"https\": f\"http://{proxy_auth}@{proxy_host}:{proxy_port}/\",\n",
        "        \"http\": f\"http://{proxy_auth}@{proxy_host}:{proxy_port}/\",\n",
        "    }\n",
        "\n",
        "    headers = {\"X-Crawlera-Profile\": \"desktop\"}\n",
        "\n",
        "    page = HTMLSession().get(\n",
        "        url, headers=headers, verify=True, allow_redirects=True, timeout=8.0)\n",
        "\n",
        "    if page.ok:\n",
        "        return BeautifulSoup(page.content, \"html.parser\")\n",
        "    else:\n",
        "        page = HTMLSession().get(\n",
        "            url,\n",
        "            headers=headers,\n",
        "            proxies=proxies,\n",
        "            verify=cert,\n",
        "            allow_redirects=True,\n",
        "            timeout=8.0,\n",
        "        )\n",
        "        return BeautifulSoup(page.content, \"html.parser\")\n",
        "\n",
        "\n",
        "def get_image_lists(url):\n",
        "    \"\"\"\n",
        "    This function returns all images but sort based on likelyhood to be a logo.\n",
        "    \"\"\"\n",
        "\n",
        "    soup = get_soup(url)\n",
        "    body = soup.find(\"body\")\n",
        "    header_images = []\n",
        "    body_images = []\n",
        "    domain = urlparse(url).netloc\n",
        "\n",
        "\n",
        "    if \"www.\" in domain:\n",
        "        domain = domain.replace(\"www.\", \"\")\n",
        "    domain =urlparse(domain).path.split(\".\")[0]\n",
        "\n",
        "\n",
        "    ## For every website, get all images,\n",
        "    # but sort based on likelyhood to be a logo\n",
        "    # (i.e. header_images > body_images (with the more logo likely to be at the start of body_images vs the end))\n",
        "    src_attrs = [\"src\", \"data-src\"]\n",
        "\n",
        "    if body and body.find(\"img\"):\n",
        "        add_to_end = []\n",
        "\n",
        "        for i in body.find_all(\"img\"):\n",
        "            # Get body>header images (with 'logo' in)\n",
        "            if i.findParents(\"header\"):\n",
        "                for src in src_attrs:\n",
        "                  if i.get(src) and \"logo\" in i.get(src).lower():\n",
        "                      header_images.append(i.get(src))\n",
        "\n",
        "            # Scrape all body images (with 'logo' in, not inc. body>header images)\n",
        "            elif not i.findParents(\"header\"):\n",
        "                for src in src_attrs:\n",
        "                    if i.get(src) and \"logo\" in str(i).lower():\n",
        "                        body_images.append(i.get(src))\n",
        "                    elif i.get(src) and domain in str(i).lower():\n",
        "                        body_images.append(i.get(src))\n",
        "\n",
        "            # Scrape all other body images\n",
        "            for src in src_attrs:\n",
        "                if i.get(src) and i.get(src) not in header_images + body_images:\n",
        "                    add_to_end.append(i.get(src))\n",
        "        body_images += add_to_end\n",
        "\n",
        "    return (header_images, body_images)  # return a tuple contains lists of image urls\n",
        "\n",
        "\n",
        "def filter_images(url, image_url):\n",
        "  if (\n",
        "      image_url is not None\n",
        "      and image_url.startswith(\"http\") == False\n",
        "      and image_url.startswith(\"/\") == False  ### example : img/logo@3x.png\n",
        "  ):\n",
        "      image_url = url + \"/\" + image_url\n",
        "\n",
        "  if image_url is not None and image_url.startswith(\"//\"):  ### //img/logo@3x.png\n",
        "      image_url = image_url.replace(\"//\", \"/\")\n",
        "\n",
        "  if (\n",
        "      image_url is not None\n",
        "      and image_url.startswith(\"http\") == False\n",
        "      and image_url.startswith(\"/\")\n",
        "  ):  ### example :/img/logo@3x.png\n",
        "      image_url = url + image_url\n",
        "\n",
        "  return image_url\n",
        "\n",
        "\n",
        "def gcs_upload(image_url, domain, file_name, gcs_root, bucket_name):\n",
        "\n",
        "    if image_url.endswith(\"svg\"):\n",
        "      filename = f\"{file_name}.svg\"  ## save image in its own format\n",
        "    else:\n",
        "      filename = f\"{file_name}.png\"\n",
        "\n",
        "    if image_url is not None and HTMLSession().get(image_url).status_code == 200:    #image is not None and \n",
        "      \n",
        "      byteImgIO = io.BytesIO()\n",
        "      print(time.time())\n",
        "      byteImg = Image.open(HTMLSession().get(image_url, timeout=3).content)\n",
        "      byteImg.save(byteImgIO, filename)\n",
        "      print(time.time())\n",
        "      upload_to_gcs(bucket_name, domain, filename)\n",
        "      gcs_logo_url = gcs_root + bucket_name + \"/\" + domain + \"/\" + filename\n",
        "      return gcs_logo_url\n",
        "\n",
        "      # byteImgIO.seek(0)\n",
        "      # byteImg = byteImgIO.read()\n",
        "      \n",
        "     \n",
        "      \n",
        "\n",
        "\n",
        "def logo_url(url):\n",
        "    \"\"\"\n",
        "    This function only gets the most possible logo url\n",
        "    \"\"\"\n",
        "    header_images, body_images = get_image_lists(url)\n",
        "    gcs_root = \"https://storage.cloud.google.com/\"\n",
        "    bucket_name = \"company_logo_bucket\"\n",
        "    domain = urlparse(url).netloc\n",
        "\n",
        "    if \"www.\" in domain:\n",
        "        domain = domain.replace(\"www.\", \"\")\n",
        "\n",
        "    if header_images is not None and header_images != []:\n",
        "        image_url = filter_images(url, header_images[0])\n",
        "        gcs_logo_url = gcs_upload(image_url, domain, domain, gcs_root, bucket_name)\n",
        "        for indx, image in enumerate(header_images[1:]):\n",
        "           image_url = filter_images(url, image)\n",
        "           try:\n",
        "            gcs_upload(image_url, domain, indx, gcs_root, bucket_name)\n",
        "           except:\n",
        "              pass\n",
        "\n",
        "        return gcs_logo_url  \n",
        "      \n",
        "    elif body_images is not None and body_images != []:\n",
        "        image_url = filter_images(url, body_images[0])\n",
        "        gcs_logo_url = gcs_upload(image_url, domain, gcs_root, bucket_name)\n",
        "        for indx, image in enumerate(body_images[1:]):\n",
        "           image_url = filter_images(url, image)\n",
        "           try:\n",
        "            gcs_upload(image_url, domain, indx, gcs_root, bucket_name)\n",
        "           except:\n",
        "              pass\n",
        "              \n",
        "        return gcs_logo_url\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "def bq_upload_logo(url):\n",
        "\n",
        "    # credentials_path = \"/app/config/servicekey.json\"\n",
        "    # os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = credentials_path\n",
        "\n",
        "  \n",
        "    domain = urlparse(url).netloc\n",
        "\n",
        "    if \"www.\" in domain:\n",
        "        domain = domain.replace(\"www.\", \"\")\n",
        "\n",
        "    client = bigquery.Client(project=\"asteroid0\")\n",
        "    QUERY = f\"\"\"SELECT CompanyName,CompanyNumber \n",
        "                FROM `asteroid0.Ceres.ukcompanies_elastic_indext` \n",
        "                WHERE URL LIKE '{domain}'\n",
        "            \"\"\"\n",
        "\n",
        "    query_job = client.query(QUERY)\n",
        "\n",
        "    rows = query_job.result()\n",
        "    gcs_logo_url = logo_url(url)\n",
        "\n",
        "    for row in rows:\n",
        "        name = row[\"CompanyName\"]\n",
        "        company_num = row[\"CompanyNumber\"]\n",
        "        upload_to_bigquery(\n",
        "            [\n",
        "                {\n",
        "                    \"company_name\": name,\n",
        "                    \"company_url\": url,\n",
        "                    \"company_number\": company_num,\n",
        "                    \"type\": \"logo\",\n",
        "                    \"gcs_url\": gcs_logo_url,\n",
        "                    \"current_timestamp\": datetime.now(),\n",
        "                }\n",
        "            ],\n",
        "            table_id=\"CompanyLogo\",\n",
        "            )\n",
        "    return  {\"comapny\": domain, \"gcs_logo_url\": gcs_logo_url}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def multithread(urls):\n",
        "#     # start the thread pool\n",
        "#     result = []\n",
        "#     with ThreadPoolExecutor(10) as executor:\n",
        "#         # execute tasks concurrently and process results in order\n",
        "#         result = executor.map(bq_upload_logo, urls)\n",
        "#     return result"
      ],
      "metadata": {
        "id": "ugPy1DZPVTpk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.io import gbq \n",
        "# s = time.time()\n",
        "client = bigquery.Client(project=\"asteroid0\")\n",
        "QUERY = f\"\"\"SELECT URL\n",
        "            FROM `asteroid0.Ceres.ukcompanies_elastic_index`\n",
        "            LIMIT 100 \"\"\"\n",
        "\n",
        "query_job = client.query(QUERY)\n",
        "# d = time.time()\n",
        "print(d-s)\n",
        "df = gbq.read_gbq(QUERY, 'asteroid0')\n",
        "urls = df['URL'].to_list()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIOgIwQbW0y0",
        "outputId": "3467effe-b220-4e52-9023-de6d9d1f6060"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7278323173522949\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "urls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgjfkOLo7Mya",
        "outputId": "b9796e14-a574-4144-86e5-8e4768d090ce"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['http://www.whitstableoystercompany.com',\n",
              " 'http://www.gehealthcare.com',\n",
              " 'http://www.swire.com',\n",
              " 'http://www.thomasarmstrong.co.uk',\n",
              " 'http://www.ashbrooks.co.uk',\n",
              " 'http://www.fwhipkin.co.uk',\n",
              " 'http://www.newby-trust.org.uk',\n",
              " 'http://www.guycarpenter.com',\n",
              " 'http://www.west-leigh.co.uk',\n",
              " 'http://www.haldane-fisher.com',\n",
              " 'http://www.williampowell.co.uk',\n",
              " 'http://www.wescol.com',\n",
              " 'http://www.suttonsgroup.com',\n",
              " 'http://www.tamo.co.uk',\n",
              " 'http://hillfoot.com',\n",
              " 'http://www.siemensfinancialservices.co.uk',\n",
              " 'http://www.tafishbuilders.co.uk',\n",
              " 'http://www.crescent-theatre.co.uk',\n",
              " 'http://www.pcproperties.co.uk',\n",
              " 'http://www.bartlettgroup.com',\n",
              " 'http://www.apg.net',\n",
              " 'http://www.edbroking.com',\n",
              " 'http://www.chargeurs-interlining.com',\n",
              " 'http://www.shj.co.uk',\n",
              " 'http://www.wolds-gliding.org',\n",
              " 'http://www.hoaresbank.co.uk',\n",
              " 'http://www.bita.org.uk',\n",
              " 'http://www.carbank.co.uk',\n",
              " 'http://www.greenscape-grounds-maintenance.co.uk',\n",
              " 'http://www.hubacontrol.com',\n",
              " 'http://www.policyselection.com',\n",
              " 'http://www.vetrotech.com',\n",
              " 'http://www.gaeltecutilities.com',\n",
              " 'http://www.eurogentec.com',\n",
              " 'http://serinusenergy.com',\n",
              " 'http://www.flyroyalbrunei.com',\n",
              " 'http://www.henderson-group.com',\n",
              " 'http://www.chemringdefence.com',\n",
              " 'http://www.metaltex.com',\n",
              " 'http://www.peta-uk.com',\n",
              " 'http://www.deesidemarine.com',\n",
              " 'http://www.coeddarcy.co.uk',\n",
              " 'http://www.jhayward.co.uk',\n",
              " 'http://www.nottinghamshirewildlife.org',\n",
              " 'http://www.westholmeschool.com',\n",
              " 'http://www.lundbeck.com',\n",
              " 'http://www.swallows.co.uk',\n",
              " 'http://www.polyformes.co.uk',\n",
              " 'http://www.galesofllangollen.co.uk',\n",
              " 'http://www.sparkhillfreight.co.uk',\n",
              " 'http://www.metapraxis.com',\n",
              " 'http://www.libertytrading.co.uk',\n",
              " 'http://triocomputing.co.uk',\n",
              " 'http://www.successtours.com',\n",
              " 'http://www.actionsealtite.com',\n",
              " 'http://www.sonangol.co.uk',\n",
              " 'http://www.aztecaerosols.co.uk',\n",
              " 'http://www.marigoldfootcare.com',\n",
              " 'http://www.marellimotori.com',\n",
              " 'http://www.sykes-gala.com',\n",
              " 'http://www.barrittglassprint.co.uk',\n",
              " 'http://www.greenlineone.com',\n",
              " 'http://www.williams-tarr.co.uk',\n",
              " 'http://www.mla-design.co.uk',\n",
              " 'http://www.chellbrook.co.uk',\n",
              " 'http://www.uktrf.co.uk',\n",
              " 'http://www.capdyn.com',\n",
              " 'http://www.adventinternational.com',\n",
              " 'http://www.hermes.co.uk',\n",
              " 'http://starfieldproductions.co.uk',\n",
              " 'http://www.marypoppinsdaynursery.co.uk',\n",
              " 'http://www.springform.co.uk',\n",
              " 'http://www.wam-leisure.co.uk',\n",
              " 'http://www.heritagepreservation.co.uk',\n",
              " 'http://www.pmgroup.co.uk',\n",
              " 'http://www.transcom.com',\n",
              " 'http://www.fordbrook.com',\n",
              " 'http://www.haremleisure.co.uk',\n",
              " 'http://www.stainternational.com',\n",
              " 'http://www.orientalarts.org.uk',\n",
              " 'http://www.lemmingstheatre.co.uk',\n",
              " 'http://firesideshoes.co.uk',\n",
              " 'http://www.camart.co.uk',\n",
              " 'http://www.schoolfarmequestrian.co.uk',\n",
              " 'http://www.kevinjordan.com',\n",
              " 'http://www.exclusivechocolates.co.uk',\n",
              " 'http://www.macdonaldps.com',\n",
              " 'http://www.pattons.co.uk',\n",
              " 'http://www.ftcg.co.uk',\n",
              " 'http://www.johnpowellcarpets.co.uk',\n",
              " 'http://www.ashmead.com',\n",
              " 'http://www.logoscomputersystems.co.uk',\n",
              " 'http://www.cornwall-ifa.co.uk',\n",
              " 'http://www.technologyapplications.co.uk',\n",
              " 'http://www.garbos-hair.co.uk',\n",
              " 'http://www.porsche.com',\n",
              " 'http://www.tudorhousefs.co.uk',\n",
              " 'http://www.acaroofing.com',\n",
              " 'http://www.condeco.co.uk',\n",
              " 'http://www.halcyanwater.com']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(bq_upload_logo('http://www.marellimotori.com'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OD2jtvN07Xly",
        "outputId": "50ef4c63-4fbc-41a0-cc6b-6d7b87e714bb"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1653383739.428741\n",
            "{'comapny': 'marellimotori.com', 'gcs_logo_url': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for url in urls[:20]:\n",
        "  print(bq_upload_logo(url))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yA4uEx3JM1Z4",
        "outputId": "c702f729-1da1-4acb-df2c-86cc507a27d6"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'comapny': 'whitstableoystercompany.com', 'gcs_logo_url': None}\n",
            "{'comapny': 'gehealthcare.com', 'gcs_logo_url': 'https://storage.cloud.google.com/company_logo_bucket/gehealthcare.com/gehealthcare.com.png'}\n",
            "{'comapny': 'swire.com', 'gcs_logo_url': None}\n",
            "{'comapny': 'thomasarmstrong.co.uk', 'gcs_logo_url': 'https://storage.cloud.google.com/company_logo_bucket/thomasarmstrong.co.uk/thomasarmstrong.co.uk.png'}\n",
            "{'comapny': 'ashbrooks.co.uk', 'gcs_logo_url': 'https://storage.cloud.google.com/company_logo_bucket/ashbrooks.co.uk/ashbrooks.co.uk.png'}\n",
            "{'comapny': 'fwhipkin.co.uk', 'gcs_logo_url': 'https://storage.cloud.google.com/company_logo_bucket/fwhipkin.co.uk/fwhipkin.co.uk.png'}\n",
            "{'comapny': 'newby-trust.org.uk', 'gcs_logo_url': 'https://storage.cloud.google.com/company_logo_bucket/newby-trust.org.uk/newby-trust.org.uk.png'}\n",
            "{'comapny': 'guycarpenter.com', 'gcs_logo_url': None}\n",
            "{'comapny': 'west-leigh.co.uk', 'gcs_logo_url': 'https://storage.cloud.google.com/company_logo_bucket/west-leigh.co.uk/west-leigh.co.uk.png'}\n",
            "{'comapny': 'haldane-fisher.com', 'gcs_logo_url': 'https://storage.cloud.google.com/company_logo_bucket/haldane-fisher.com/haldane-fisher.com.png'}\n",
            "{'comapny': 'williampowell.co.uk', 'gcs_logo_url': 'https://storage.cloud.google.com/company_logo_bucket/williampowell.co.uk/williampowell.co.uk.png'}\n",
            "{'comapny': 'wescol.com', 'gcs_logo_url': 'https://storage.cloud.google.com/company_logo_bucket/wescol.com/wescol.com.png'}\n",
            "{'comapny': 'suttonsgroup.com', 'gcs_logo_url': 'https://storage.cloud.google.com/company_logo_bucket/suttonsgroup.com/suttonsgroup.com.svg'}\n",
            "{'comapny': 'tamo.co.uk', 'gcs_logo_url': 'https://storage.cloud.google.com/company_logo_bucket/tamo.co.uk/tamo.co.uk.png'}\n",
            "{'comapny': 'hillfoot.com', 'gcs_logo_url': 'https://storage.cloud.google.com/company_logo_bucket/hillfoot.com/hillfoot.com.svg'}\n",
            "{'comapny': 'siemensfinancialservices.co.uk', 'gcs_logo_url': None}\n",
            "{'comapny': 'tafishbuilders.co.uk', 'gcs_logo_url': None}\n",
            "{'comapny': 'crescent-theatre.co.uk', 'gcs_logo_url': 'https://storage.cloud.google.com/company_logo_bucket/crescent-theatre.co.uk/crescent-theatre.co.uk.png'}\n",
            "{'comapny': 'pcproperties.co.uk', 'gcs_logo_url': 'https://storage.cloud.google.com/company_logo_bucket/pcproperties.co.uk/pcproperties.co.uk.svg'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'comapny': 'bartlettgroup.com', 'gcs_logo_url': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "A9_sNr9cSDZr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}