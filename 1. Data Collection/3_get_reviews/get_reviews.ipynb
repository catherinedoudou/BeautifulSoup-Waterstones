{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python395jvsc74a57bd00a17921dd30452e6a2c353fb7f4d911db2807057162813f896092343d3ffd08a",
   "display_name": "Python 3.9.5  ('venv': venv)",
   "language": "python"
  },
  "metadata": {
   "interpreter": {
    "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### Get reviews\n",
    "- The root link i use here is the same as the book link, just need to add'/order/_votes/page/{page}' to it\n",
    "- To the books which have more than one page, just change the page_number on the link\n",
    "- put it inside a loop to get all reviews\n",
    "- I create functions to do different things, it works faster "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup ## load necessay libs\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\GIT\\waterstones_main\\Data\\all_extra_info.csv') # read csv to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = df[df['review_links'].notnull()] ## filter the null values\n",
    "review_df = review_df[['num_reviews', 'review_links']] #create new dataframe with new columns 'num_reviews,'review_links\n",
    "review_df['pages'] = (review_df['num_reviews'].astype(float).sort_values() / 12).apply(np.ceil)  # count how many reviews each book has\n",
    "review_df.sort_values(by='pages', ascending=False, inplace=True) # sort dataframe by number of pages, from most to least"
   ]
  },
  {
   "source": [
    "\n",
    "- loop through each url to get all reviews on each page into a dict\n",
    "- store it in a big list with each dictionary.\n",
    "- load to dataframe\n",
    "- save to pickle file\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_text(bs_find): # find the text in HTML\n",
    "    return bs_find.text if bs_find else None\n",
    "    \n",
    "\n",
    "def extract(url): ## from url to soup\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    return (url, soup)\n",
    "\n",
    "\n",
    "def transform(soup): ###scrape the review details\n",
    "    url, soup = soup[0], soup[1]\n",
    "    reviews = soup.find_all('div',class_='review-list-item-main')\n",
    "\n",
    "    for review in reviews:\n",
    "        header = find_text(review.find('h2'))\n",
    "        stars = review.find_all('div', class_='active')\n",
    "        star_rating = len(stars)\n",
    "        review_by = review.find('span', class_='review-meta').text[2:]\n",
    "        review_date = review.find('div', class_='comment-bottom').div.text\n",
    "        review_content = find_text(review.find('p', class_='p-medium')).strip()\n",
    "        \n",
    "        user_review = {'header':header,\n",
    "                    'star_rating':star_rating,\n",
    "                    'review_by':review_by,\n",
    "                    'review_date':review_date,\n",
    "                    'review_content':review_content,\n",
    "                    'url': url}\n",
    "\n",
    "        all_reviews.append(user_review)\n",
    "\n",
    "\n",
    "                                      \n",
    "def load(results): # get all_reviews make it as dataframe\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(8430, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "review_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews = []\n",
    "\n",
    "for row in review_df.itertuples():### in the for loop to loop through each book review link to get review details\n",
    "    for page in range(1, int(row.pages)+1):\n",
    "        soup = extract(row.review_links +  f'/order/_votes/page/{page}')\n",
    "        try:\n",
    "            transform(soup)\n",
    "\n",
    "        except Exception as err:   ### write any errors into a file\n",
    "            with open('errors.txt', 'a') as f:\n",
    "                f.write(f'{err}\\n')\n",
    "\n",
    "\n",
    "load(all_reviews).to_pickle('all_reviews.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df = pd.read_pickle('all_reviews.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df.drop_duplicates(inplace=True)  ## drop the overlapped rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df.to_pickle('books_reviews.pkl')### save to pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}