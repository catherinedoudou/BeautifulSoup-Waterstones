{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python391jvsc74a57bd06a1981fa971648e9004e5f4fa4e4d93b00923e8ca5447f264d1f66cbe5218b44",
   "display_name": "Python 3.9.1 64-bit ('learning': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### More information"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Getting more information based on the book links I got from previous script\n",
    "- use the book link to get catergory, publisher, pages, isbn, datepublished,e,g\n",
    "- get the data, save them to pickle file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup  ###import necessary libraries\n",
    "from numpy import random\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "\n",
    "\n",
    "def write_to_pickle(name, data):  ### his function takes whatever list/dict into dataframe and convert it into a pickle file\n",
    "    file_name = f'{name}.pkl'\n",
    "    \n",
    "    if file_name in os.listdir():\n",
    "        return 'File already exist'\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_pickle(file_name)"
   ]
  },
  {
   "source": [
    "## Get Extra Information\n",
    "After getting all the url for each book,use the url to get extra Information, e.g reviews, publisher"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(datetime.now())\n",
    "\n",
    "\n",
    "def find_text(bs_find):  ### This function find text in the \n",
    "    return bs_find.text if bs_find else None\n",
    "\n",
    "\n",
    "def more_info(each_book_url):  ### This function takes book link as input\n",
    "    r = requests.get(each_book_url)\n",
    "    link_page = BeautifulSoup(r.text)\n",
    "    book = {}\n",
    "\n",
    "    book['categories'] = find_text(link_page.find('div', class_='breadcrumbs span12'))\n",
    "    book['contributors'] = find_text(link_page.find('span',class_='contributors'))\n",
    "    book['pages'] = find_text(link_page.find('span', itemprop='numberOfPages'))\n",
    "    book['datePublished'] = link_page.find(itemprop='datePublished').get('content')\n",
    "    book['availability'] = find_text(link_page.find('span', id='scope_offer_availability'))\n",
    "    book['description'] = find_text(link_page.find('div', itemprop='description'))\n",
    "    book['publisher'] = find_text(link_page.find('span', itemprop='publisher'))\n",
    "    book['isbn'] = find_text(link_page.find('span',itemprop='isbn'))\n",
    "    book['weight'] = find_text(link_page.find('span',itemprop='weight'))\n",
    "    book['num_reviews'] = link_page.find(itemprop='reviewCount').get('content')\n",
    "    book['num_stars'] = link_page.find(itemprop='ratingValue').get('content')\n",
    "    book['media_reviews'] = find_text(link_page.find('div', class_='show-desc'))\n",
    "\n",
    "\n",
    "    if book['num_reviews']:\n",
    "        a_href= link_page.find('div', id='reviews').find(\"a\", href=True)\n",
    "        book['review_links'] = 'https://www.waterstones.com' + a_href['href']\n",
    "        \n",
    "        \n",
    "    book['book_link'] = each_book_url\n",
    "\n",
    "    return book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = r'../Data/books_basic.pkl'  ### take in the data got from perivous\n",
    "df = pd.read_pickle(x) \n",
    "urls = df['book_link'].to_list()  ### book_link to a list\n",
    "\n",
    "book_details = []\n",
    "for idx, url in enumerate(urls):\n",
    "    try:  \n",
    "        book_details.append(more_info(url))\n",
    "        if (idx + 1) % 10000 == 0:  ### every 10000 books write to pickle\n",
    "            write_to_pickle(f'books_{idx+1}', book_details) \n",
    "            book_details = []\n",
    "\n",
    "    except Exception as err:  ### catch the errors\n",
    "        with open('errors.txt', 'a') as f:\n",
    "            f.write(f'{idx}:{err}\\n')\n",
    "\n",
    "print('FINISH:', datetime.now())"
   ]
  }
 ]
}