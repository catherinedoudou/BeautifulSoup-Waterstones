{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd063fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d",
   "display_name": "Python 3.9.4 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### This way you can do the scraping faster"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bs4'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f566b511527f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBea\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcsv\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconcurrent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfutures\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msqlalchemy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'bs4'"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from csv import reader\n",
    "from datetime import datetime\n",
    "import concurrent.futures\n",
    "import sqlalchemy\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def find_text(bs_find):\n",
    "    return bs_find.text if bs_find else None\n",
    "\n",
    "\n",
    "def more_info(each_book_url):\n",
    "    r = requests.get(each_book_url)\n",
    "    link_page = BeautifulSoup(r.text)\n",
    "    book = {}\n",
    "\n",
    "    book['categories'] = find_text(link_page.find('div', class_='breadcrumbs span12'))\n",
    "    book['contributors'] = find_text(link_page.find('span',class_='contributors'))\n",
    "    book['pages'] = find_text(link_page.find('span', itemprop='numberOfPages'))\n",
    "    book['datePublished'] = link_page.find(itemprop='datePublished').get('content')\n",
    "    book['availability'] = find_text(link_page.find('span', id='scope_offer_availability'))\n",
    "    book['description'] = find_text(link_page.find('div', itemprop='description'))\n",
    "    book['publisher'] = find_text(link_page.find('span', itemprop='publisher'))\n",
    "    book['isbn'] = find_text(link_page.find('span',itemprop='isbn'))\n",
    "    book['weight'] = find_text(link_page.find('span',itemprop='weight'))\n",
    "    book['num_stars'] = link_page.find(itemprop='ratingValue').get('content')\n",
    "    book['media_reviews'] = find_text(link_page.find('div', class_='show-desc'))\n",
    "\n",
    "    book['num_reviews'] = link_page.find(itemprop='reviewCount').get('content')\n",
    "\n",
    "    if book['num_reviews']:\n",
    "        a_href= link_page.find('div', id='reviews').find(\"a\", href=True)\n",
    "        book['review_links'] = 'https://www.waterstones.com' + a_href['href']\n",
    "        \n",
    "    book['book_link'] = each_book_url\n",
    "    return book\n",
    "\n",
    "  \n",
    "def transform(url):\n",
    "  try:  \n",
    "      book_details.append(more_info(url))\n",
    "  except Exception as err:\n",
    "      with open('/content/drive/MyDrive/scraping/errors.txt', 'a') as f:\n",
    "          f.write(f'{url}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.now())\n",
    "\n",
    "urls = []\n",
    "book_details = []\n",
    "start = 0\n",
    "stop = 10\n",
    "\n",
    "\n",
    "with open('/content/drive/MyDrive/scraping/links.csv', 'r') as f:\n",
    "  csv_reader = reader(f)\n",
    "  for row in csv_reader:\n",
    "    urls.append(row[1])\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "  executor.map(transform, urls[start:stop])\n",
    "\n",
    "conn = \"postgresql://USERNAME:PASSWORD@HOSTNAME:5432/postgres\"\n",
    "engine = sqlalchemy.create_engine(conn)\n",
    "df.to_sql('TABLENAME', engine, if_exists='append', method='multi')\n",
    "\n",
    "print(datetime.now())"
   ]
  }
 ]
}